{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook focus on testing the Document Retreive Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import gc\n",
    "from IPython.display import display, Markdown \n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import ctypes\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    OFFLINE = False #True # for Test offline environment\n",
    "    USE_8BIT = True #\n",
    "    USE_4BIT = False #\n",
    "    USE_LLAMA3 = False # \n",
    "    USE_GEMMA2 = False # \n",
    "    USE_QWEN = False # \n",
    "    USE_DEEPSEEK = True # \n",
    "    USE_DEEPSCALE = False # \n",
    "\n",
    "    TASK_GEN = True # for generative Text output task (suitable for RAG project)\n",
    "    TEST_LLM = True\n",
    "    USE_HUGGINGFACE = True # Pull model from Huggingface model hub\n",
    "    USE_LMSTUIDO = False # for local LLM framework \n",
    "    USE_OLLAMA = False # for OLLAMA local LLM framework \n",
    "    USE_VLLM = False # for VLLM  LLM framework\n",
    "\n",
    "    # mulitlingual LLM model \n",
    "    model1 = \"meta-llama/Llama-3.2-3B-Instruct\"  # llama3.2  3B-Instruct\n",
    "\n",
    "    model2 =  \"google/gemma-2-2b-it\" # gemma 2 9B (mulitlingual)\n",
    "    model3 = \"Qwen/Qwen2.5-3B-Instruct\" # Qwen 3B (mulitlingual)\n",
    "    model4 = 'Qwen/Qwen2.5-7B-Instruct' # Qwen 7B (mulitlingual)\n",
    "    model5 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # DeepSeek Distill 1.5B (mulitlingual)\n",
    "    model6 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # DeepSeek Distill 7B (mulitlingual)\n",
    "    model7 = \"agentica-org/DeepScaleR-1.5B-Preview\"\n",
    "    \n",
    "    model8 = \"protectai/deberta-v3-base-prompt-injection-v2\"\n",
    "    \n",
    "    # for VLM model\n",
    "    vlmModel1 = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    vlmModel2 = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "\n",
    "    # Mult Embedding model\n",
    "    embedModel1 = 'intfloat/multilingual-e5-small' # for embedding model support chinese\n",
    "    embedModel2 = \"intfloat/multilingual-e5-large-instruct\"\n",
    "    embedModel3 = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\" # for embedding model support chinese\n",
    "    embedModel4 = \"Alibaba-NLP/gte-multilingual-base\" # for embedding model support chinese\n",
    "    embedModel5 = \"BAAI/bge-m3\" # for multilingual embedding model\n",
    "    embedModel6 = \"jinaai/jina-embeddings-v3\"\n",
    "    \n",
    "\n",
    "\n",
    "    FEW_SHOT_TEST= False#True\n",
    "    USE_WANDB = True#True # for  LLM evalution and debug , track fine tuning performance\n",
    "\n",
    "    USE_DEEPEVAL = True#False # for LLM evalution   \n",
    "    USE_TRAIN =  False #True #False#True Much be use GPU for Training \n",
    "    \n",
    "    # For VectorDB selection\n",
    "    USE_FAISS = False#True # For RAG VectorDB\n",
    "    USE_CHROMA = True #False #True #False # for RAG VectorDF\n",
    "    USE_PINECONE = False#True#False #True # for RAG VectorDF\n",
    "    USE_WEAVIATE = False#True #False # for RAG VectorDF\n",
    "    USE_MILVUS = False#True              # for RAG VectorDF\n",
    "\n",
    "    # for LLM fine tuning\n",
    "    maxTrainData = 200#3500#5000 #10000#5000 #10000\n",
    "    maxEvalData = 20#100 # 20 \n",
    "\n",
    "    # LLM parameters\n",
    "    reportTo =\"none\"\n",
    "    topK = 40\n",
    "    topP = 1.0\n",
    "    temperature = 0.6 #0.5\n",
    "    repetition_penalty = 1.05 # 1.1\n",
    "    maxOutToken = 1024#180 #100\n",
    "\n",
    "    \n",
    "    maxToken=  512#768#512#768 # 512 for test only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnsonhk88/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/johnsonhk88/.local/lib/python3.10/site-packages/langchain_huggingface/chat_models/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_huggingface.chat_models.huggingface import (\n",
      "2025-02-28 00:35:11.883566: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 00:35:12.059109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740674112.101088  233243 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740674112.118341  233243 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 00:35:12.224512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import (AutoTokenizer, \n",
    "                          BitsAndBytesConfig,\n",
    "                         AutoModelForCausalLM,\n",
    "                         AutoModelForSequenceClassification,\n",
    "                         TrainingArguments)\n",
    "\n",
    "from langchain_community.document_loaders import (TextLoader,\n",
    "                                                  PyMuPDFLoader,\n",
    "                                                  PyPDFDirectoryLoader,\n",
    "                                                  PyPDFLoader)\n",
    "\n",
    "# from langchain.document_loaders import PyPDFDirectoryLoader # old version of document loader\n",
    "\n",
    "from langchain.prompts.prompt import  PromptTemplate\n",
    "\n",
    "from langchain_community.vectorstores import FAISS #, Chroma,  Pinecone # old version of VectorStore\n",
    "\n",
    "\n",
    "\n",
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter,\n",
    "                                      CharacterTextSplitter ,\n",
    "                                       SentenceTransformersTokenTextSplitter)   \n",
    "\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings # huggingfaceEmbedding deprecated , please use sentencetransformers \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "\n",
    "import evaluate\n",
    "import trulens\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearMemory():\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HuggingFace Hub Access for download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "huggingfaceToken = os.getenv(\"HuggingFace\") #get huggeface token from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingfaceToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjohnsonhk88\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/johnsonhk88/.netrc\n"
     ]
    }
   ],
   "source": [
    "if CFG.USE_WANDB:\n",
    "    # train report to  W&B tool\n",
    "    import wandb\n",
    "    reportTo= \"wandb\"\n",
    "    my_secret = os.getenv(\"wandb_api_key\") \n",
    "    wandb.login(key=my_secret) # login \n",
    "else: \n",
    "    reportTo = \"none\"# None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract PDF File Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use Simple pypdf extract simple pdf text data. But can't extract complex layout and extract position picture information from pdf, for test propose\n",
    "##### We start use different AI model extract complex data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design AI model for Table detection, Image detection model, and Text detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Test Document path \n",
    "pdfFilePath1 = \"../test-document/Attention .pdf\"\n",
    "pdfFilePath2 = \"../test-document/yolo.pdf\"\n",
    "pdfDir = \"../test-document\"\n",
    "\n",
    "bankStatementDir = \"../bank-statement-document/\"\n",
    "bankStatementSamples =  \"../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from  PyPDF2 import PdfReader\n",
    "from pypdf import PdfReader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader  #langchain pyMuPDF loader not perfect extract different f\n",
    "from langchain_docling import DoclingLoader\n",
    "\n",
    "\n",
    "from pdf2image import convert_from_path #for pdf to image convert\n",
    "import cv2\n",
    "import pymupdf\n",
    "from PIL import Image\n",
    "\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pymupdf library different formats extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain pymupdf library different formats extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pymuPDFLoader = PyMuPDFLoader(bankStatementSamples)\n",
    "loadPDF1 =pymuPDFLoader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Issue Date:\n",
      "Period:\n",
      "Account Activity\n",
      "Date\n",
      "Payment Type\n",
      "Paid In\n",
      "Paid Out\n",
      "Balance\n",
      "Your Account Statement\n",
      "Detail\n",
      "Note:\n",
      "Print Form\n",
      "Save Form\n",
      "Reset Form\n",
      "<Branch Name>\n",
      "231 Valley Farms Street \n",
      "Santa Monica, CA \n",
      "bickslowbank@domain.com\n",
      "mm/dd/yyyy\n",
      "mm/dd/yyyy to mm/dd/yyyy\n",
      "111-234-567-890  \n",
      "Bit Manufacturing Ltd\n",
      "2450 Courage St, STE 108\n",
      "Brownsville, TX 78521\n",
      "Balance Brought Forward\n",
      "8,313.30\n",
      "mm/dd/yyyy Fast Payment\n",
      "Amazon\n",
      "132.30\n",
      "8,181.00\n",
      "mm/dd/yyyy BACS\n",
      "eBAY Trading Co.\n",
      "515.22\n",
      "7,665.78\n",
      "mm/dd/yyyy Fast Payment\n",
      "Morrisons Petrol\n",
      "80.00\n",
      "7,585.78\n",
      "mm/dd/yyyy BACS\n",
      "Business Loan\n",
      "20,000.00\n",
      "27,585.78\n",
      "mm/dd/yyyy BACS\n",
      "Jumes White Media\n",
      "2,416.85\n",
      "25,168.93\n",
      "mm/dd/yyyy Fast Payment\n",
      "ATM High Street\n",
      "100.00\n",
      "25,068.93\n",
      "mm/dd/yyyy BACS\n",
      "Accorn Advertising Studios\n",
      "150.00\n",
      "24,918.93\n",
      "Fast Payment\n",
      "mm/dd/yyyy\n",
      "Marriott Hotels\n",
      "177.00\n",
      "24,741.93\n",
      "mm/dd/yyyy Fast Payment\n",
      "Abelio Scotrail Ltd\n",
      "122.22\n",
      "24,619.71\n",
      "mm/dd/yyyy Fast Payment\n",
      "Cheque 000234\n",
      "1,200.00\n",
      "23,419.71\n",
      "mm/dd/yyyy Int. Bank\n",
      "Interest Paid\n",
      "9.33\n",
      "23,429.04\n",
      "mm/dd/yyyy DD\n",
      "OVO Energy\n",
      "270.00\n",
      "23,159.04\n",
      "mm/dd/yyyy BACS\n",
      "Toyota Online\n",
      "10,525.40\n",
      "12,633.64\n",
      "mm/dd/yyyy BACS\n",
      "HMRC\n",
      "1,000.00\n",
      "11,633.64\n",
      "mm/dd/yyyy DD\n",
      "OVLA\n",
      "280.00\n",
      "11,353.64\n",
      "mm/dd/yyyy EBP\n",
      "Michael Kor Salary\n",
      "1,554.00\n",
      "9,799.64\n",
      "mm/dd/yyyy DD\n",
      "BOS Mastercard\n",
      "4,000.00\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "5,799.64\n",
      "' metadata={'source': '../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf', 'file_path': '../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.7', 'title': 'Bank Statement Template 2 - TemplateLab.xlsx', 'author': 'HFO Desktop', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Microsoft: Print To PDF', 'creationDate': \"D:20200703162211+08'00'\", 'modDate': \"D:20200703193651+08'00'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(loadPDF1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use pymupdf original library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextFromPage(page):\n",
    "    '''get text by pymupdf\n",
    "    '''\n",
    "    text = page.get_text()\n",
    "    return text\n",
    "\n",
    "def extractTableFromPage(page):\n",
    "    tabs = page.find_tables()\n",
    "    print(f\"{len(tabs.tables)} found on {page}\") # display number of found tables\n",
    "    for i, tab in enumerate(tabs.tables):\n",
    "        print(f\"Table {i+1} : {tab.extract()}\")\n",
    "    return tabs\n",
    "\n",
    "def extractImageFromPage(page):\n",
    "    image_list = page.get_images()\n",
    "    imginfo = page.get_image_info()\n",
    "    print(imginfo)\n",
    "    print(image_list)\n",
    "    return image_list\n",
    "\n",
    "def showPageText(docs):\n",
    "    for page in docs:\n",
    "        # print(page)\n",
    "        result = extractTextFromPage(page)\n",
    "        # result = extractTableFromPage(page) #test extract table\n",
    "        # result = extractImageFromPage(page) #test extract image\n",
    "        print(type(result))\n",
    "\n",
    "# loadPDF1 = pymupdf.open(pdfFilePath1)\n",
    "\n",
    "# extract Table \n",
    "# for page in loadPDF1:\n",
    "#     tabs = page.find_tables()\n",
    "#     if tabs.tables:\n",
    "#         print(tabs[0].extract())\n",
    "#         # print(tabs[0].extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "page9 = loadPDF1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf', 'file_path': '../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf', 'page': 0, 'total_pages': 1, 'format': 'PDF 1.7', 'title': 'Bank Statement Template 2 - TemplateLab.xlsx', 'author': 'HFO Desktop', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Microsoft: Print To PDF', 'creationDate': \"D:20200703162211+08'00'\", 'modDate': \"D:20200703193651+08'00'\", 'trapped': ''}, page_content='Issue Date:\\nPeriod:\\nAccount Activity\\nDate\\nPayment Type\\nPaid In\\nPaid Out\\nBalance\\nYour Account Statement\\nDetail\\nNote:\\nPrint Form\\nSave Form\\nReset Form\\n<Branch Name>\\n231 Valley Farms Street \\nSanta Monica, CA \\nbickslowbank@domain.com\\nmm/dd/yyyy\\nmm/dd/yyyy to mm/dd/yyyy\\n111-234-567-890  \\nBit Manufacturing Ltd\\n2450 Courage St, STE 108\\nBrownsville, TX 78521\\nBalance Brought Forward\\n8,313.30\\nmm/dd/yyyy Fast Payment\\nAmazon\\n132.30\\n8,181.00\\nmm/dd/yyyy BACS\\neBAY Trading Co.\\n515.22\\n7,665.78\\nmm/dd/yyyy Fast Payment\\nMorrisons Petrol\\n80.00\\n7,585.78\\nmm/dd/yyyy BACS\\nBusiness Loan\\n20,000.00\\n27,585.78\\nmm/dd/yyyy BACS\\nJumes White Media\\n2,416.85\\n25,168.93\\nmm/dd/yyyy Fast Payment\\nATM High Street\\n100.00\\n25,068.93\\nmm/dd/yyyy BACS\\nAccorn Advertising Studios\\n150.00\\n24,918.93\\nFast Payment\\nmm/dd/yyyy\\nMarriott Hotels\\n177.00\\n24,741.93\\nmm/dd/yyyy Fast Payment\\nAbelio Scotrail Ltd\\n122.22\\n24,619.71\\nmm/dd/yyyy Fast Payment\\nCheque 000234\\n1,200.00\\n23,419.71\\nmm/dd/yyyy Int. Bank\\nInterest Paid\\n9.33\\n23,429.04\\nmm/dd/yyyy DD\\nOVO Energy\\n270.00\\n23,159.04\\nmm/dd/yyyy BACS\\nToyota Online\\n10,525.40\\n12,633.64\\nmm/dd/yyyy BACS\\nHMRC\\n1,000.00\\n11,633.64\\nmm/dd/yyyy DD\\nOVLA\\n280.00\\n11,353.64\\nmm/dd/yyyy EBP\\nMichael Kor Salary\\n1,554.00\\n9,799.64\\nmm/dd/yyyy DD\\nBOS Mastercard\\n4,000.00\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n5,799.64\\n')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test OCR base PDF file extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # covert pdf to image\n",
    "# imgPdf =convert_from_path(pdfFilePath1)\n",
    "# len(imgPdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgPdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OCR on an image\n",
    "# page = imgPdf[2] # get page 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from PDF by OCR extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for page in imgPdf:\n",
    "#     text = pytesseract.image_to_string(page)\n",
    "#     print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pytesseract.image_to_string(page)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract page text with coordiate \n",
    "# def extractPageImgTextWithCoord(pageImg):\n",
    "#     data = pytesseract.image_to_data(pageImg, output_type=\"dict\")\n",
    "#     print(data.keys())\n",
    "#     numBox = len(data['level'])\n",
    "#     cv2img =cv2.cvtColor(np.array(pageImg), cv2.COLOR_RGB2BGR)\n",
    "#     # draw bounding box \n",
    "#     for i in range(numBox):\n",
    "#         (x, y, w, h) = (data['left'][i], data['top'][i], data['width'][i], data['height'][i])\n",
    "#         cv2.rectangle(cv2img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    \n",
    "#     # cv2.imshow(\"Image\", cv2img) # show image with bounding box in window\n",
    "#     # # cv2.imshow(\"test\", image)\n",
    "#     # cv2.waitKey(0)\n",
    "#     # cv2.destroyAllWindows()\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(cv2img)\n",
    "#     plt.show()\n",
    "#     return data\n",
    "\n",
    "\n",
    "# extractData = extractPageImgTextWithCoord(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.imshow(\"page\", page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pytesseract.image_to_string(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader1 =PdfReader(pdfFilePath1)\n",
    "# type(reader1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numPage = len(reader1.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currentPage=reader1.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(currentPage.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getPDFText(pdfDoc):\n",
    "#     '''\n",
    "#     get pdf text from pdf docs\n",
    "#     '''\n",
    "#     text=\"\" \n",
    "#     pdf_reader= PdfReader(pdfDoc) #read pdf file\n",
    "#     for page in pdf_reader.pages: # loop through pdf pages \n",
    "#             text+= page.extract_text() # extract text from page and add to text variable\n",
    "#     return  text # return text variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use PyPDFDirectoryLoader from Langchain load PDF files from DirectoryLoader\n",
    "<https://python.langchain.com/v0.2/docs/how_to/document_loader_pdf/#using-pypdf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPDFDocs(directory):\n",
    "    '''\n",
    "    use PyPDFDirectoryLoader to extract pdf document from directory\n",
    "    '''\n",
    "    loader = PyPDFDirectoryLoader(directory) \n",
    "    docs = loader.load()\n",
    "    return docs #text # return text variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../bank-statement-document/Dummy-Bank-Statement.pdf', 'page': 0}, page_content=\"Dummy Bank Statement \\n BankName:People'sTrustBankCustomerName:JohnA.DoeAccountNumber:123-456-789StatementPeriod:July1,2023- July31,2023Address:123MapleStreet,Anytown,AT12345\\nAccountSummaryOpeningBalance:$5,000.00ClosingBalance:$4,250.00\\nTransactions\\nDate Description Withdrawals Deposits Balance\\n07/01/2023OpeningBalance - - $5,000.00\\n07/02/2023ElectricBillPayment $250.00 - $4,750.00\\n07/05/2023GroceryStore $150.00 - $4,600.00\\n07/08/2023SalaryDeposit - $1,000.00 $5,600.00\\n07/12/2023OnlineShopping- Z-Mart $100.00 - $5,500.00\\n07/15/2023CashWithdrawal- ATM $200.00 - $5,300.00\\nCopyright @SampleTemplates.com\\n\"),\n",
       " Document(metadata={'source': '../bank-statement-document/Dummy-Bank-Statement.pdf', 'page': 1}, page_content=\"2\\n07/18/2023CarInsurancePremium $350.00 - $4,950.00\\n07/22/2023CoffeeShop $20.00 - $4,930.00\\n07/25/2023GasStation $50.00 - $4,880.00\\n07/28/2023WaterBillPayment $300.00 - $4,580.00\\n07/30/2023GymMembershipFee $80.00 - $4,500.00\\n07/31/2023\\nMovieStreamingServiceSubscription\\n$50.00 - $4,450.00\\n07/31/2023MonthlyMaintenanceFee $200.00 - $4,250.00\\nFees&Charges\\n● MonthlyMaintenanceFee:$200.00● ATMWithdrawalFee(non-network):Includedintransactions\\nNotes:\\n● Keepyourbankstatementforyourrecords.● Reviewyourstatementregularlytomonitoryouraccountactivity.● ContactPeople'sTrustBankimmediatelyifyounoticeanyunauthorizedtransactions.\\nCopyright @SampleTemplates.com\\n\"),\n",
       " Document(metadata={'source': '../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf', 'page': 0}, page_content='Issue Date:\\nPeriod:\\nAccount Activity\\nDate Payment Type Paid In Paid Out Balance\\nYour Account Statement\\nDetail\\nNote:')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs =getPDFDocs(bankStatementDir)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../bank-statement-document/Dummy-Bank-Statement.pdf', 'page': 0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Data Analysis\n",
    "\n",
    "## level 1 : Layout Analysis, extract/analysis document component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a model\n",
    "yoloModel = YOLO(\"yolov8n.pt\")  # load an official model\n",
    "# model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# Validate the model\n",
    "# metrics = yoloModel.val()  # no arguments needed, dataset and settings remembered\n",
    "# metrics.box.map  # map50-95\n",
    "# metrics.box.map50  # map50\n",
    "# metrics.box.map75  # map75\n",
    "# metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page3 = imgPdf[2]\n",
    "# page3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = yoloModel(page3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### level2 each component AI model extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 3 Analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "# Quantized Config for GPU support only\n",
    "if CFG.USE_8BIT:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit = True,\n",
    "        bnb_8bit_quant_type=\"nf8\",\n",
    "        bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_8bit_use_double_quant=True # Activate nested quantization for 8-bit base models (double quantization)\n",
    "\n",
    "        )\n",
    "    \n",
    "elif CFG.USE_4BIT:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(CFG.model2, token=huggingfaceToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.USE_LMSTUIDO:\n",
    "    # Point to the local server\n",
    "    from langchain.llms import OpenAI\n",
    "    import openai\n",
    "    llmModel = \"LMStudio\"\n",
    "    model = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "else: # load model from huggingface\n",
    "    if device.type == \"cuda\": # use 7b/8b/9b model gain performance\n",
    "        if CFG.USE_LLAMA3:\n",
    "            modelSel = CFG.model1\n",
    "            llmModel = \"llama3_8b\"\n",
    "        \n",
    "        elif CFG.USE_GEMMA2:\n",
    "            modelSel = CFG.model4\n",
    "            llmModel = \"gemma2_9b\"\n",
    "        \n",
    "        elif CFG.USE_QWEN:\n",
    "            modelSel = CFG.model3\n",
    "            llmModel = \"qwen2.5_3b\"\n",
    "\n",
    "        elif CFG.USE_DEEPSEEK:\n",
    "            modelSel = CFG.model5\n",
    "            llmModel = \"deepseek_1.5b\"\n",
    "\n",
    "        else: \n",
    "            modelSel = CFG.model2\n",
    "            llmModel = 'gemma_2b'\n",
    "        \n",
    "        if CFG.TASK_GEN:\n",
    "            model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  \n",
    "                                                 quantization_config= bnb_config ,\n",
    "                                                #  torch_dtype=torch.bfloat16, \n",
    "                                                 token=huggingfaceToken)\n",
    "\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  \n",
    "                                                 quantization_config= bnb_config, token=huggingfaceToken)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(modelSel, token=huggingfaceToken , torch_dtype=torch.bfloat16) # inital tokenizer\n",
    "        tokenizer.padding_side = \"right\"\n",
    "    \n",
    "\n",
    "    else: # for cpu select smaller model\n",
    "        modelSel = CFG.model2\n",
    "        llmModel = 'gemma_2b'\n",
    "        if CFG.TASK_GEN:\n",
    "            model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\", token=huggingfaceToken)\n",
    "\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\", token=huggingfaceToken)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(modelSel, token=huggingfaceToken) # inital tokenizer\n",
    "        tokenizer.padding_side = \"right\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear8bitLt(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear8bitLt(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del model\n",
    "# del model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deepseek_1.5b'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grudModel = AutoModelForSequenceClassification.from_pretrained( CFG.model7, token=huggingfaceToken, \n",
    "#                                                                device_map=device, torch_type=None)\n",
    "                                                               \n",
    "\n",
    "#                                                             #    quantization_config=bnb_config)\n",
    "\n",
    "# grudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delModel():\n",
    "    global model, tokenizer\n",
    "    del model\n",
    "    del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del grudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "templatePrompt1 = \"\"\"Question: {question}.\\nOnly require given final result in JSON format with key 'answer'\n",
    "            \"\"\"\n",
    "templatePrompt2 = \"Answer the user Question.\\n###\\n{format_instructions}\\n###\\nQuestion: {query}\\n\"\n",
    "\n",
    "templatePrompt3 = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, Answer is not available in the context, don't provide the wrong answer\\n\\n\n",
    "    Context: {context}\\n\n",
    "    Question: {question}\\n\n",
    "    \"\"\"\n",
    "\n",
    "templatePrompt4 = \"\"\"\n",
    "Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in \n",
    "provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "CONTEXT: {context}\n",
    "Provide answer and rethinking multiple step by step from Question: {question}\n",
    "\"\"\"\n",
    "templatePrompt5 = \"\"\"\n",
    "you are act as Mathematician, solve the math problem reasonable and logical from given question follow the requirement as below:\n",
    "CONTEXT: {context}\n",
    "Provide answer and rethinking multiple step by step from Question: {question}\n",
    "Only Output answer in json format with key \"answer\" and \"explanation\" \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "you are act as Mathematician, solve the math problem reasonable and logical from given question follow the requirement as below:\n",
      "CONTEXT: {context}\n",
      "Provide answer and rethinking multiple step by step from Question: {question}\n",
      "Only Output answer in json format with key \"answer\" and \"explanation\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(templatePrompt5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generateResponse(query, maxOutToken=CFG.maxOutToken, topP=CFG.topP,\n",
    "                          topK=CFG.topK, temperature = CFG.temperature\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Direct send message to LLM model, get response\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    startTime = time.time()\n",
    "    inputIds = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    response = model.generate(**inputIds,\n",
    "                              do_sample=True,\n",
    "                              top_p=topP,\n",
    "                              top_k = topK,\n",
    "                              temperature=temperature,\n",
    "                              max_new_tokens= maxOutToken,\n",
    "                             )\n",
    "    print(f\"Time Taken : {time.time() - startTime}\")\n",
    "    # return tokenizer.decode(response[0][len(inputIds[\"input_ids\"]):], skip_special_tokens = True)\n",
    "    generatedIDs = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputIds.input_ids, response)\n",
    "    ]\n",
    "    # print(f\"GeneratedIDs : {generatedIDs}\")\n",
    "    return tokenizer.batch_decode(generatedIDs, skip_special_tokens=True)[0]\n",
    "    \n",
    "\n",
    "def generateChatInstMsg(instruct, query):\n",
    "    return   [\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instruct,\n",
    "            },\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": query},\n",
    "        ]\n",
    "\n",
    "async def generateChatResponse(chatMsg ,maxOutToken=CFG.maxOutToken, topP=CFG.topP,\n",
    "                          topK=CFG.topK, temperature = CFG.temperature):\n",
    "    \"\"\"\n",
    "    send chat message to LLM\n",
    "    \"\"\"\n",
    "    startTime = time.time()\n",
    "    text = tokenizer.apply_chat_template(chatMsg, \n",
    "                                         tokenize=False, \n",
    "                                         add_generation_prompt=True)\n",
    "    inputIDs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    response = model.generate(**inputIDs, \n",
    "                             do_sample=True,  #enable for Temperature \n",
    "                             top_p= topP,\n",
    "                             top_k = topK,\n",
    "                             temperature = temperature,\n",
    "                             max_new_tokens=maxOutToken,\n",
    "                             repetition_penalty= CFG.repetition_penalty)\n",
    "    print(f\"Time Taken : {time.time() - startTime}\")\n",
    "    generatedIDs = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(inputIDs.input_ids, response)\n",
    "    ]\n",
    "    # print(f\"GeneratedIDs : {generatedIDs}\")\n",
    "    return tokenizer.batch_decode(generatedIDs, skip_special_tokens=True)[0]\n",
    "    # return tokenizer.decode(response[0][len(inputIDs[\"input_ids\"]):], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple parser for extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from  json.decoder import JSONDecodeError\n",
    "if CFG.TASK_GEN:\n",
    "\n",
    "    def isInteger(text):\n",
    "        try:\n",
    "            if int(text) >= 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def llmJSONparser(txt, key=\"answer\"):\n",
    "        \"\"\"\n",
    "        try to get answer from LLM response , expect in JSON format, \n",
    "        \"\"\"\n",
    "        try:\n",
    "            subText = txt.split(\"{\") # split several {} in list \n",
    "            for txtSeg in subText: # loop in list to find answer\n",
    "                end = txtSeg.find(\"}\") # find end position in text segment\n",
    "                sub = txtSeg[:end] #subsring with {} context\n",
    "                print(sub)\n",
    "                temp = sub.replace(\"*\", \"\") # remove * symbol\n",
    "                temp = temp.replace(\"\\\"\", \"\") # reomve \\\" symbol\n",
    "                temp = temp.lower() # convert to lower case\n",
    "                answerloc = temp.find(key) # find key word \"answer\" position\n",
    "                if answerloc != -1:\n",
    "                    print(f\"find answer location : {answerloc}\")\n",
    "                    newTxt = temp[answerloc:] # substring start answer\n",
    "#                   print(\"Temp: \", temp)\n",
    "                    subTxt = newTxt.split(\"\\n\")\n",
    "                    #       print(subTxt)\n",
    "                    rel =subTxt[0][len(key):].strip() # get answer value with remove space\n",
    "                    rel= rel.replace(',', '') # remove , symbol\n",
    "                    print(rel)\n",
    "                    return rel\n",
    "                \n",
    "            return None # can't find answer\n",
    "        except :\n",
    "            print(f\"\"\"Error LLM JSON parser input txt {txt}\"\"\" )\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "\n",
    "    def getLLMAnswerParser(txt, key=\"answer:\"):\n",
    "        \"\"\"\n",
    "        when json parser failure, seem answer not JSON format, \n",
    "        use \"answer\" for key word search final answer \n",
    "        \"\"\"\n",
    "         # find answer  \n",
    "        temp = txt.replace(\"*\", \"\") # remove * symbol\n",
    "        temp = temp.replace(\"\\\"\", \"\") # reomve \"\" symbol\n",
    "        temp = temp.lower() # convert to lower case\n",
    "        # find answer key word\n",
    "        start = temp.find(key)\n",
    "        print(f\"Start loc: {start}\")\n",
    "        subStr = temp[start:]\n",
    "        if start != -1:\n",
    "            subTxt = subStr.split(\"\\n\")\n",
    "           #print(subTxt)\n",
    "            rel =subTxt[0][len(key):].strip() # get answer value with remove space\n",
    "            rel= rel.replace(',', '') # remove , symbol\n",
    "            print(rel)\n",
    "            return rel\n",
    "    \n",
    "        print(subStr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add parser  to control extreact data from  LLM Structure Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import (StrOutputParser, \n",
    "                                           JsonOutputParser,\n",
    "                                           PydanticOutputParser,\n",
    "                                          )\n",
    "# for LLM structure output\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "# from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken : 71.93657112121582\n",
      " What is its difference with deep learning? What is the difference between unsupervised and supervised learning? What is the difference between parametric and non-parametric learning?\n",
      "Okay, let's start by understanding the basics of machine learning. It's a subset of AI that focuses on developing algorithms that allow computers to learn from data without being explicitly programmed. So, the core idea is that machines can improve their performance as they gain more data.\n",
      "\n",
      "Now, thinking about the difference between machine learning and deep learning. Deep learning is a subset of machine learning that specifically uses neural networks with multiple layers to model and solve complex problems. Unlike traditional machine learning algorithms, which are often based on statistical models, deep learning algorithms are inspired by the structure and function of the human brain. This makes deep learning particularly effective for tasks like image recognition, natural language processing, and other complex pattern recognition tasks.\n",
      "\n",
      "Next, the difference between unsupervised and supervised learning. In supervised learning, the algorithm is trained on labeled data, where each data point is associated with a specific output or label. The goal is to learn a mapping from inputs to outputs. For example, if you have a dataset of emails labeled as spam or not spam, a supervised learning algorithm would learn to classify new emails into these categories based on the patterns in the training data.\n",
      "\n",
      "On the other hand, unsupervised learning deals with unlabeled data. The algorithm tries to find hidden patterns or intrinsic structures in the data without any predefined outputs. Common unsupervised techniques include clustering (like k-means), dimensionality reduction (like PCA), and association rule learning. For instance, a clustering algorithm might group similar customer behaviors together without knowing the specific categories yet.\n",
      "\n",
      "Moving on to the difference between parametric and non-parametric learning. Parametric models assume a certain form of the underlying data distribution, which is defined by a finite number of parameters. These models are often computationally efficient and can be trained with less data because they make strong assumptions about the data. For example, linear regression is a parametric model because it assumes a linear relationship between features and the target variable.\n",
      "\n",
      "In contrast, non-parametric models do not make strong assumptions about the underlying data distribution. Instead, they allow the data to define the model complexity, which means they can adapt better to the data but may require more data to achieve good performance. Techniques like k-nearest neighbors (k-NN) and decision trees are examples of non-parametric models. They are more flexible and can capture complex patterns, but they might be less interpretable compared to parametric models.\n",
      "\n",
      "I think I have a good grasp of these concepts now. To summarize:\n",
      "\n",
      "- **Machine Learning**: Focuses on developing algorithms that enable computers to learn from data, without being explicitly programmed. It encompasses various types like supervised, unsupervised, and reinforcement learning.\n",
      "\n",
      "- **Deep Learning**: A subset of machine learning that uses neural networks with multiple layers to learn representations from data. It's particularly effective for complex tasks like image and speech recognition.\n",
      "\n",
      "- **Unsupervised Learning**: Uses unlabeled data to find patterns and structures, such as grouping similar data points or reducing dimensionality.\n",
      "\n",
      "- **Supervised Learning**: Uses labeled data to train models to predict outputs based on inputs, such as classification or regression tasks.\n",
      "\n",
      "- **Parametric Learning**: Models that assume a specific form of the data distribution, requiring fewer parameters and less data.\n",
      "\n",
      "- **Non-Parametric Learning**: Models that do not assume a specific data distribution, allowing the data to define model complexity, which can lead to better performance on complex tasks but may require more data.\n",
      "\n",
      "This understanding should help in applying these concepts to various problems and algorithms in machine learning.\n",
      "</think>\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "**Machine Learning Overview:**\n",
      "- **Definition:** A subset of AI that focuses on developing algorithms enabling machines to learn from data without explicit programming.\n",
      "- **Key Concepts:** Supervised, unsupervised, and reinforcement learning.\n",
      "\n",
      "**Difference Between Machine Learning and Deep Learning:**\n",
      "- **Machine Learning:** Broad category using various algorithms for tasks like classification, regression, clustering, and dimensionality reduction.\n",
      "- **Deep Learning:** Subset of machine learning using neural networks with multiple layers for complex tasks such as image recognition, natural language processing.\n",
      "\n",
      "**Difference Between Unsupervised and Supervised Learning:**\n",
      "- **Supervised Learning:** Uses labeled data to train models to predict outputs (e.g., classification, regression).\n",
      "- **Unsupervised Learning:** Uses unlabeled data to find patterns and structures (e.g., clustering, dimensionality reduction).\n",
      "\n",
      "**Difference Between Parametric and Non-Parametric Learning:**\n",
      "- **Parametric Learning:** Models assuming a specific data distribution (e.g., linear regression), requiring fewer parameters and less data.\n",
      "- **Non-Parametric Learning:** Models without assumptions about data distribution (e.g., k-nearest neighbors), allowing data to define complexity, potentially requiring more data.\n",
      "\n",
      "**Summary:**\n",
      "- **Machine Learning** encompasses various types of learning, including unsupervised and parametric, and is a subset of AI.\n",
      "- **Deep Learning**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if CFG.TEST_LLM:\n",
    "    ret =await generateResponse(\"What is Machine Learning?\",    maxOutToken=1024, topP=0.95,\n",
    "                          topK=10, temperature = 0.6)\n",
    "    print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " What is its difference with deep learning? What is the difference between unsupervised and supervised learning? What is the difference between parametric and non-parametric learning?\n",
       "Okay, let's start by understanding the basics of machine learning. It's a subset of AI that focuses on developing algorithms that allow computers to learn from data without being explicitly programmed. So, the core idea is that machines can improve their performance as they gain more data.\n",
       "\n",
       "Now, thinking about the difference between machine learning and deep learning. Deep learning is a subset of machine learning that specifically uses neural networks with multiple layers to model and solve complex problems. Unlike traditional machine learning algorithms, which are often based on statistical models, deep learning algorithms are inspired by the structure and function of the human brain. This makes deep learning particularly effective for tasks like image recognition, natural language processing, and other complex pattern recognition tasks.\n",
       "\n",
       "Next, the difference between unsupervised and supervised learning. In supervised learning, the algorithm is trained on labeled data, where each data point is associated with a specific output or label. The goal is to learn a mapping from inputs to outputs. For example, if you have a dataset of emails labeled as spam or not spam, a supervised learning algorithm would learn to classify new emails into these categories based on the patterns in the training data.\n",
       "\n",
       "On the other hand, unsupervised learning deals with unlabeled data. The algorithm tries to find hidden patterns or intrinsic structures in the data without any predefined outputs. Common unsupervised techniques include clustering (like k-means), dimensionality reduction (like PCA), and association rule learning. For instance, a clustering algorithm might group similar customer behaviors together without knowing the specific categories yet.\n",
       "\n",
       "Moving on to the difference between parametric and non-parametric learning. Parametric models assume a certain form of the underlying data distribution, which is defined by a finite number of parameters. These models are often computationally efficient and can be trained with less data because they make strong assumptions about the data. For example, linear regression is a parametric model because it assumes a linear relationship between features and the target variable.\n",
       "\n",
       "In contrast, non-parametric models do not make strong assumptions about the underlying data distribution. Instead, they allow the data to define the model complexity, which means they can adapt better to the data but may require more data to achieve good performance. Techniques like k-nearest neighbors (k-NN) and decision trees are examples of non-parametric models. They are more flexible and can capture complex patterns, but they might be less interpretable compared to parametric models.\n",
       "\n",
       "I think I have a good grasp of these concepts now. To summarize:\n",
       "\n",
       "- **Machine Learning**: Focuses on developing algorithms that enable computers to learn from data, without being explicitly programmed. It encompasses various types like supervised, unsupervised, and reinforcement learning.\n",
       "\n",
       "- **Deep Learning**: A subset of machine learning that uses neural networks with multiple layers to learn representations from data. It's particularly effective for complex tasks like image and speech recognition.\n",
       "\n",
       "- **Unsupervised Learning**: Uses unlabeled data to find patterns and structures, such as grouping similar data points or reducing dimensionality.\n",
       "\n",
       "- **Supervised Learning**: Uses labeled data to train models to predict outputs based on inputs, such as classification or regression tasks.\n",
       "\n",
       "- **Parametric Learning**: Models that assume a specific form of the data distribution, requiring fewer parameters and less data.\n",
       "\n",
       "- **Non-Parametric Learning**: Models that do not assume a specific data distribution, allowing the data to define model complexity, which can lead to better performance on complex tasks but may require more data.\n",
       "\n",
       "This understanding should help in applying these concepts to various problems and algorithms in machine learning.\n",
       "</think>\n",
       "\n",
       "**Final Answer:**\n",
       "\n",
       "**Machine Learning Overview:**\n",
       "- **Definition:** A subset of AI that focuses on developing algorithms enabling machines to learn from data without explicit programming.\n",
       "- **Key Concepts:** Supervised, unsupervised, and reinforcement learning.\n",
       "\n",
       "**Difference Between Machine Learning and Deep Learning:**\n",
       "- **Machine Learning:** Broad category using various algorithms for tasks like classification, regression, clustering, and dimensionality reduction.\n",
       "- **Deep Learning:** Subset of machine learning using neural networks with multiple layers for complex tasks such as image recognition, natural language processing.\n",
       "\n",
       "**Difference Between Unsupervised and Supervised Learning:**\n",
       "- **Supervised Learning:** Uses labeled data to train models to predict outputs (e.g., classification, regression).\n",
       "- **Unsupervised Learning:** Uses unlabeled data to find patterns and structures (e.g., clustering, dimensionality reduction).\n",
       "\n",
       "**Difference Between Parametric and Non-Parametric Learning:**\n",
       "- **Parametric Learning:** Models assuming a specific data distribution (e.g., linear regression), requiring fewer parameters and less data.\n",
       "- **Non-Parametric Learning:** Models without assumptions about data distribution (e.g., k-nearest neighbors), allowing data to define complexity, potentially requiring more data.\n",
       "\n",
       "**Summary:**\n",
       "- **Machine Learning** encompasses various types of learning, including unsupervised and parametric, and is a subset of AI.\n",
       "- **Deep Learning**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg1 = generateChatInstMsg(\"You are a intelligent Chatbot response to answer user query\", \n",
    "                           \"What is LLM model use case?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken : 35.163382053375244\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, so I'm trying to understand what LLM models are and how they're used. I've heard the term before in tech discussions, but I don't really know much about it. Let me start by breaking down the question: \"What is LLM model use case?\" \n",
       "\n",
       "First, I think \"LLM\" stands for Large Language Model. I remember hearing about GPT in my studies, and that must be an example of an LLM. So, LLMs are these AI models designed to generate human-like text. They're used in various ways, right?\n",
       "\n",
       "I should probably list out some common use cases. Maybe something like chatbots? Yeah, I've used chat apps before where people just type a question and get an answer. That sounds like a chatbot application using an LLM. But wait, are there other types of use cases besides chatbots?\n",
       "\n",
       "Maybe personalized recommendations? I've heard of recommendation systems, like Netflix suggesting shows you'll like. If an LLM can understand your preferences, it might help with that too. But does it really do that, or is it more about generating recommendations based on data?\n",
       "\n",
       "Natural language processing (NLP) comes to mind. I think NLP is all about understanding and generating human language. So, maybe LLMs are used in NLP tasks, like translation, summarization, or even detecting fake news. That could be another use case.\n",
       "\n",
       "Also, I've seen articles about LLMs being used in healthcare. Maybe predicting diseases based on symptoms? Or in finance, automating trading strategies? Those could be examples too. I wonder how precise they need to be in those fields.\n",
       "\n",
       "Another thought: customer service automation. If businesses have a lot of interactions, an LLM could help with scheduling calls, handling multiple issues simultaneously, and providing real-time responses. That would save a lot of time compared to manually handling each case.\n",
       "\n",
       "I'm also thinking about education. Maybe an LLM can provide instant feedback on quizzes, explain concepts in a way that's easier to understand, or even help with homework. It could make learning more efficient.\n",
       "\n",
       "In terms of business operations, supply chain management might use LLMs to optimize routes for delivery, manage inventory levels, or predict demand based on historical data. That could lead to better planning and cost savings.\n",
       "\n",
       "I'm curious about the ethical implications. Using LLMs for sensitive areas like hate speech detection could be problematic. There's a risk of misuse or bias. How do companies ensure their LLMs are fair"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = await generateChatResponse(msg1, maxOutToken=512, topP=0.95,\n",
    "                          topK=20, temperature = 0.6)\n",
    "Markdown(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, so I'm trying to understand what LLM models are and how they're used. I've heard the term before in tech discussions, but I don't really know much about it. Let me start by breaking down the question: \"What is LLM model use case?\" \n",
       "\n",
       "First, I think \"LLM\" stands for Large Language Model. I remember hearing about GPT in my studies, and that must be an example of an LLM. So, LLMs are these AI models designed to generate human-like text. They're used in various ways, right?\n",
       "\n",
       "I should probably list out some common use cases. Maybe something like chatbots? Yeah, I've used chat apps before where people just type a question and get an answer. That sounds like a chatbot application using an LLM. But wait, are there other types of use cases besides chatbots?\n",
       "\n",
       "Maybe personalized recommendations? I've heard of recommendation systems, like Netflix suggesting shows you'll like. If an LLM can understand your preferences, it might help with that too. But does it really do that, or is it more about generating recommendations based on data?\n",
       "\n",
       "Natural language processing (NLP) comes to mind. I think NLP is all about understanding and generating human language. So, maybe LLMs are used in NLP tasks, like translation, summarization, or even detecting fake news. That could be another use case.\n",
       "\n",
       "Also, I've seen articles about LLMs being used in healthcare. Maybe predicting diseases based on symptoms? Or in finance, automating trading strategies? Those could be examples too. I wonder how precise they need to be in those fields.\n",
       "\n",
       "Another thought: customer service automation. If businesses have a lot of interactions, an LLM could help with scheduling calls, handling multiple issues simultaneously, and providing real-time responses. That would save a lot of time compared to manually handling each case.\n",
       "\n",
       "I'm also thinking about education. Maybe an LLM can provide instant feedback on quizzes, explain concepts in a way that's easier to understand, or even help with homework. It could make learning more efficient.\n",
       "\n",
       "In terms of business operations, supply chain management might use LLMs to optimize routes for delivery, manage inventory levels, or predict demand based on historical data. That could lead to better planning and cost savings.\n",
       "\n",
       "I'm curious about the ethical implications. Using LLMs for sensitive areas like hate speech detection could be problematic. There's a risk of misuse or bias. How do companies ensure their LLMs are fair"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CFG.TEST_LLM:\n",
    "    display(Markdown(ret)) # display in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken : 65.12370324134827\n",
      " So, please provide the step by step explanation to the question and put the final answer in the correct format.\n",
      "<think>\n",
      "Okay, so I need to figure out what Machine Learning is. I've heard the term before, but I'm not entirely sure what it really means. Let me start by breaking it down. \n",
      "\n",
      "First, I know that Machine Learning is a subset of Artificial Intelligence. AI, as I understand it, is about creating systems that can perform tasks that typically require human intelligence. But Machine Learning is a bit different. It's about training algorithms to make decisions or predictions without being explicitly programmed.\n",
      "\n",
      "So, how does it work? I think it involves data. The system is trained on a dataset, which includes examples of the inputs it needs to process and the corresponding outputs. This data is split into training and testing sets. The algorithm learns from the training data, adjusting its parameters to minimize the error between its predictions and the actual outputs in the training set. \n",
      "\n",
      "Once the model is trained, it's used to make predictions on new, unseen data. This is where the model's generalization comes into play. It's supposed to make accurate predictions without being overfitted to the training data. Overfitting, as I recall, is when a model learns the noise in the training data instead of the underlying pattern, leading to poor performance on new data.\n",
      "\n",
      "I also remember something about supervised and unsupervised learning. In supervised learning, the model is trained on labeled data, where each example is paired with the correct output. This allows the model to learn the relationship between inputs and outputs. In unsupervised learning, there are no labels, and the model has to find patterns or intrinsic structures in the data on its own. \n",
      "\n",
      "Another concept I'm a bit fuzzy on is feature engineering. This involves selecting and transforming the input data to improve the model's performance. It's not just about having the right data but also having the right features that can help the model learn effectively.\n",
      "\n",
      "I'm also curious about the applications of Machine Learning. It seems to be used in a wide range of fields, from healthcare and finance to autonomous vehicles and recommendation systems. Each application uses Machine Learning to analyze data, identify patterns, and make data-driven decisions.\n",
      "\n",
      "I wonder about the different types of Machine Learning models. There are classification models, which predict categories, like spam detection or image recognition. Regression models predict continuous values, such as stock prices or temperature. Clustering models group similar data points together, useful for customer segmentation. And reinforcement learning involves agents learning to make decisions by interacting with an environment to maximize rewards.\n",
      "\n",
      "I'm trying to think about some specific examples. For instance, facial recognition systems use Machine Learning to analyze images and recognize faces. Autonomous cars use sensors and ML algorithms to make decisions in real-time. In healthcare, ML can predict disease outbreaks or recommend personalized treatments based on patient data.\n",
      "\n",
      "I also think about the challenges in Machine Learning. One major challenge is data quality. The data needs to be clean, relevant, and sufficient for training a model. If the data is biased, the model might perform worse on certain groups. Another challenge is computational resources. Training complex models can be resource-intensive, requiring powerful hardware and significant computing time.\n",
      "\n",
      "There's also the issue of interpretability. Many Machine Learning models, especially deep learning ones, are often considered \"black boxes.\" This means it's hard to understand how the model arrived at a particular decision. For applications where transparency is important, like in finance or healthcare, this can be a problem.\n",
      "\n",
      "I'm curious about the future of Machine Learning. It's possible that advancements in AI and data science will lead to more sophisticated and efficient ML models. Integration with other technologies like IoT (Internet of Things) and big data will be crucial for ML applications in the near future.\n",
      "\n",
      "In summary, Machine Learning is a powerful subset of AI that uses data and algorithms to enable systems to perform tasks without explicit programming. It involves training models on data, using supervised and unsupervised learning, feature engineering, and various model types. It has a wide range of applications and faces challenges like data quality and interpretability. The future looks promising with ongoing advancements in technology.\n",
      "</think>\n",
      "\n",
      "{\n",
      "  \"answer\": {\n",
      "    \"description\": \"Machine Learning is a subset of Artificial Intelligence that involves training algorithms to make decisions or predictions based on data. It uses datasets split into training and testing sets, allowing models to learn patterns and generalize to new data. Common applications include healthcare, finance, and autonomous vehicles. Key challenges include data quality and interpretability, with potential future advancements in technology enhancing its capabilities.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "query = \"What is Machine Learning?\"\n",
    "newPrompt = PromptTemplate(template=templatePrompt1,\n",
    "                           input_variables=[\"question\"])\n",
    "finalPrompt = newPrompt.format(\n",
    "                question=query    \n",
    "            )\n",
    "rel = await generateResponse(finalPrompt, maxOutToken=1024)\n",
    "print(rel)\n",
    "# jsonTxt = getLLMAnswerParser(rel, key=\"answer\")\n",
    "# print(f\"Question : {query}\\nResponse Answer: {jsonTxt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defin RAG alogrithm\n",
    "USE_SIMPLE_RAG = True#True# True # simple similairy approach \n",
    "USE_RERANK = False #False # advance RAG with Re-Ranking \n",
    "USE_QUERY_EXPANSION = False   # advance RAG with \n",
    "USE_EMBEDDING_ADAPER = False\n",
    "if CFG.USE_WANDB: # define wandb RAG project name\n",
    "    if USE_SIMPLE_RAG:\n",
    "        wandbRAGProject = \"ai-bank-statement-simple-rag\"\n",
    "    elif USE_RERANK:\n",
    "        wandbRAGProject = \"ai-bank-statement-re-ranking\"\n",
    "    elif USE_QUERY_EXPANSION:\n",
    "        wandbRAGProject = \"ai-bank-statement-query-expansion\"\n",
    "    elif USE_EMBEDDING_ADAPER:\n",
    "        wandbRAGProject = \"ai-bank-statement-embedding-adapter\"\n",
    "    else:\n",
    "        wandbRAGProject = \"ai-bank-statement-simple-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.19.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/johnsonhk88/Big-Data-Disk2/AI-Bank-Statement-Document-Automation-By-LLM-And-Personal-Finanical-Analysis-Prediction/src/dev/wandb/run-20250224_232738-mvmtf7uy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/johnsonhk88/ai-bank-statement-simple-rag/runs/mvmtf7uy' target=\"_blank\">peachy-dust-21</a></strong> to <a href='https://wandb.ai/johnsonhk88/ai-bank-statement-simple-rag' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/johnsonhk88/ai-bank-statement-simple-rag' target=\"_blank\">https://wandb.ai/johnsonhk88/ai-bank-statement-simple-rag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/johnsonhk88/ai-bank-statement-simple-rag/runs/mvmtf7uy' target=\"_blank\">https://wandb.ai/johnsonhk88/ai-bank-statement-simple-rag/runs/mvmtf7uy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CFG.USE_WANDB:\n",
    "     # Start a new wandb run\n",
    "    runTask1 = wandb.init(project=wandbRAGProject, job_type=\"generation\", anonymous=\"allow\")\n",
    "    # define W&B Table\n",
    "    wandbCol1 =  [\"model\", \"question\",  \"llm_generate\", \"llm_answer\"]\n",
    "    wandbRAGTable =wandb.Table(columns=wandbCol1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text To Langchain Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertText2Document(content, source):\n",
    "    \"\"\"\n",
    "    Convert text to Document object\n",
    "    \"\"\"\n",
    "    doc = Document(\n",
    "        page_content= content, \n",
    "        metadata= {\"sources\": source}\n",
    "        )\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inital Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"cuda\":\n",
    "    model_kwargs = {\"device\": \"cuda\"}\n",
    "    multiProcess=  False#True #  for multi-GPU\n",
    "else:\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    multiProcess= False\n",
    "def embeddingModelInit(modelName):\n",
    "        embed =  HuggingFaceEmbeddings(model_name=modelName, model_kwargs= model_kwargs, multi_process=multiProcess)#initial embedding model \n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embeddingModelInit(CFG.embedModel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector= embedding.embed_query(\"Hello, how are you?\")\n",
    "len(vector) # checking vector length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text split into Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter,\n",
    "                                      CharacterTextSplitter,\n",
    "                                      MarkdownHeaderTextSplitter,\n",
    "                                      HTMLHeaderTextSplitter,\n",
    "                                      HTMLSectionSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textSplitterByText(txt, chunkSize= 512, overlap=20, separators=[\"\\n\\n\"]):\n",
    "    \"\"\"\n",
    "    Split text by chunk size\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunkSize,\n",
    "        chuck_overlap=overlap,\n",
    "        separators=separators,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        # start_with_newline=True,\n",
    "        # add_start_index=True,\n",
    "        # add_end_index=True,\n",
    "    )\n",
    "    splittedTxt = splitter.split_text(txt)\n",
    "    return splittedTxt\n",
    "\n",
    "\n",
    "def textSplitterByDocs(docs, chunkSize= 512, overlap= 20, separators=[\"\\n\\n\"]):\n",
    "    textSplitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunkSize,\n",
    "        chunk_overlap = overlap,\n",
    "        separators=separators,\n",
    "        length_function = len,\n",
    "        is_separator_regex =False\n",
    "    )\n",
    "    splitted = textSplitter.split_documents(docs)\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SIMPLE_RAG:\n",
    "    textSplitter = RecursiveCharacterTextSplitter(\n",
    "                                chunk_size= 800, #1500,\n",
    "                                chunk_overlap=20, #100,\n",
    "                                add_start_index=True,\n",
    "                                separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "                                  length_function=len,\n",
    "                                is_separator_regex=False)\n",
    "else: # advance RAG possabile use adv method\n",
    "    textSplitter = CharacterTextSplitter(chunk_size=1500, \n",
    "                                             chunk_overlap=100,\n",
    "                                             length_function=len,\n",
    "                                             separator=\"\\n\\n\",\n",
    "                                             is_separator_regex=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTextDataFromText(text):\n",
    "    chunk = textSplitter.split_text(text)\n",
    "    return textSplitter.create_documents(chunk) # create docs\n",
    "    \n",
    "def chunkText(docs, chunkSize=800, chunkOverlap=50):\n",
    "    # Split text into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunkSize, chunk_overlap=chunkOverlap)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    return chunks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rawTxt1)\n",
    "# docList = splitTextDataFromText(rawTxt1)\n",
    "# len(docList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../bank-statement-document/Dummy-Bank-Statement.pdf', 'page': 0}, page_content=\"Dummy Bank Statement \\n BankName:People'sTrustBankCustomerName:JohnA.DoeAccountNumber:123-456-789StatementPeriod:July1,2023- July31,2023Address:123MapleStreet,Anytown,AT12345\\nAccountSummaryOpeningBalance:$5,000.00ClosingBalance:$4,250.00\\nTransactions\\nDate Description Withdrawals Deposits Balance\\n07/01/2023OpeningBalance - - $5,000.00\\n07/02/2023ElectricBillPayment $250.00 - $4,750.00\\n07/05/2023GroceryStore $150.00 - $4,600.00\\n07/08/2023SalaryDeposit - $1,000.00 $5,600.00\\n07/12/2023OnlineShopping- Z-Mart $100.00 - $5,500.00\\n07/15/2023CashWithdrawal- ATM $200.00 - $5,300.00\\nCopyright @SampleTemplates.com\"),\n",
       " Document(metadata={'source': '../bank-statement-document/Dummy-Bank-Statement.pdf', 'page': 1}, page_content=\"2\\n07/18/2023CarInsurancePremium $350.00 - $4,950.00\\n07/22/2023CoffeeShop $20.00 - $4,930.00\\n07/25/2023GasStation $50.00 - $4,880.00\\n07/28/2023WaterBillPayment $300.00 - $4,580.00\\n07/30/2023GymMembershipFee $80.00 - $4,500.00\\n07/31/2023\\nMovieStreamingServiceSubscription\\n$50.00 - $4,450.00\\n07/31/2023MonthlyMaintenanceFee $200.00 - $4,250.00\\nFees&Charges\\n● MonthlyMaintenanceFee:$200.00● ATMWithdrawalFee(non-network):Includedintransactions\\nNotes:\\n● Keepyourbankstatementforyourrecords.● Reviewyourstatementregularlytomonitoryouraccountactivity.● ContactPeople'sTrustBankimmediatelyifyounoticeanyunauthorizedtransactions.\\nCopyright @SampleTemplates.com\"),\n",
       " Document(metadata={'source': '../bank-statement-document/Bank-Statement-Template-2-TemplateLab.pdf', 'page': 0}, page_content='Issue Date:\\nPeriod:\\nAccount Activity\\nDate Payment Type Paid In Paid Out Balance\\nYour Account Statement\\nDetail\\nNote:')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkedDocs = textSplitterByDocs(docs, chunkSize=1500, overlap=50, separators=[\"\\n\\n\", \"\\n\"]) # chunk text into documents\n",
    "chunkedDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunkedDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../bank-statement-document/Dummy-Bank-Statement.pdf', 'page': 0}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkedDocs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Document in VectorDB\n",
    "### langchain new version required different vector database install different vector database libraries and Call API\n",
    "### for Chroma\n",
    "<https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/>\n",
    "\n",
    "### for Pinecone\n",
    "<https://python.langchain.com/v0.2/docs/integrations/vectorstores/pinecone/>\n",
    "\n",
    "### for FAISS\n",
    "<https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentIDs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 196 ms, sys: 17.4 ms, total: 213 ms\n",
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from uuid import uuid4\n",
    "from langchain_chroma import Chroma\n",
    "import time\n",
    "# VectorDB inital and store document into Vector DB\n",
    "if CFG.USE_CHROMA:\n",
    "        \n",
    "        def chormaDBInit(collectionName, embedding, persitDict=\"./chroma_db\"):\n",
    "            chromadb = Chroma(collection_name=collectionName,\n",
    "                                embedding_function=embedding, \n",
    "                                persist_directory=persitDict)\n",
    "            return chromadb\n",
    "        \n",
    "        def addDocIDs(ids):\n",
    "            documentIDs.extend(ids)\n",
    "\n",
    "        def removeDocIDs(idx):\n",
    "            documentIDs.remove(idx)\n",
    "\n",
    "        def generateDocIDs(numDoc=1):\n",
    "            uuids= []\n",
    "            for _ in range(numDoc):\n",
    "                uuids.append( str(uuid4()))\n",
    "                addDocIDs(uuids)\n",
    "             # print(f\"UUIDS : {uuids}\")\n",
    "            return uuids\n",
    "        \n",
    "        def saveToChroma(db, chuck: list[Document]):\n",
    "            db.add_documents(chuck)\n",
    "\n",
    "        async def saveToChromaIDAsyc(db, chuck: list[Document]):\n",
    "            startTime = time.time()\n",
    "            uuids = generateDocIDs(len(chuck))\n",
    "            await db.aadd_documents(chuck, ids=uuids)\n",
    "            print(f\"Time Taken:  {time.time() - startTime}\")\n",
    "        \n",
    "\n",
    "        def saveToChromaID(db, chuck: list[Document]):\n",
    "            # generate document ID \n",
    "            uuids = generateDocIDs(len(chuck))\n",
    "            db.add_documents(chuck, ids=uuids)\n",
    "\n",
    "        def delChroma(db, idx):\n",
    "            db.delete(ids=idx)\n",
    "\n",
    "        def delChromaAll(db):\n",
    "            db.delete(ids=documentIDs)\n",
    "\n",
    "\n",
    "        def vectorDBsimilaritySearchWithScores(db, query, k =3):\n",
    "            results = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "            return results\n",
    "\n",
    "        def vectorDBsimilaritySearch(db, query, k=3):\n",
    "            results = db.similarity_search(query, k=k)\n",
    "            return results\n",
    "\n",
    "\n",
    "        def vectorDBsimilaritySearchByVector(db, query, embed, k=3 ):\n",
    "            results = db.similarity_search_by_vector(\n",
    "            embedding=embed.embed_query(query), k=k)\n",
    "            return results\n",
    "\n",
    "        def vectorDBSimilaritySearchByMMR(db, query, k=3, fetchK=20):\n",
    "            results = db.max_marginal_relevance_search(query=query, k=k, fetch_k=fetchK)\n",
    "            return results\n",
    "\n",
    "        db = chormaDBInit(\"bank-statement\", embedding, persitDict=\"./chroma_db\")\n",
    "        saveToChromaID(db, chunkedDocs)\n",
    "        # db = Chroma.from_documents(documents= chunkedDocs, embedding=embedding, persist_directory=\"./chroma_db\")\n",
    "elif CFG.USE_FAISS:\n",
    "        db = FAISS.from_documents(documents =chunkedDocs, embedding= embedding)\n",
    "        print(db.index.ntotal) # number of total index size\n",
    "elif CFG.USE_PINECONE:\n",
    "        from pinecone import Pinecone, ServerlessSpec\n",
    "        from langchain_pinecone import PineconeVectorStore\n",
    "        os.environ['PINECONE_API_KEY'] = os.getenv(\"PINECONE_API_KEY\")# \n",
    "        pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "\n",
    "        import time\n",
    "\n",
    "        index_name = \"langchainvector2\"  # piecond  db index name , can change if desired\n",
    "\n",
    "        existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "        if index_name not in existing_indexes:\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=1024,#768,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "            )\n",
    "            while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "                time.sleep(1)\n",
    "\n",
    "        index = pc.Index(index_name)\n",
    "        # Connect to Pinecone index and insert the chunked docs as contents\n",
    "        db =PineconeVectorStore.from_documents(chunkedDocs, embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7c753491b9a0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Query for Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Yolo?\"\n",
    "# results = db.similarity_search(query)\n",
    "\n",
    "\n",
    "results = vectorDBsimilaritySearchWithScores(db, query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 9, 'source': '../test-document/yolo.pdf'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup RAG Top K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SIMPLE_RAG:\n",
    "        num_docs= 2 # set max top k=  3 rank similarity  \n",
    "elif  USE_RERANK:\n",
    "        num_docs =15 # re-ranking use highter order \n",
    "else: \n",
    "        num_docs =3\n",
    "\n",
    "# set retriever\n",
    "retriever  = db.as_retriever( \n",
    "                search_type=\"mmr\",  # Also test \"similarity\"\n",
    "                search_kwargs={\"k\": num_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x72d454fc9b10>, search_type='mmr', search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for VectorDB with retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[Document(metadata={'page': 9, 'source': '../test-document/yolo.pdf'}, page_content='Farhadi. You only look once: Uniﬁed, real-time ob-\\nject detection, 2016. Supplied as additional material\\nhttps://arxiv.org/pdf/1506.02640.pdf . 7\\n[13] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\\nimprovement. CoRR , abs/1804.02767, 2018. 4\\n[14] Dillon Reis, Jacqueline Hong, Jordan Kupec, and Ahmad\\nDaoudi. Real time ﬂying object detection code repository.\\n1\\n[15] Zion Market Research. Global drone market size to register\\ncagr of about 38.75 percent over 2023-2030, 2023. March\\n15 2023. 1\\n[16] Jacob Solawetz and Francesco. What is yolov8? the ultimate\\nguide., 2023. 04-30-2023. 1, 5, 8\\n[17] Emma Soteriou. Ukraine ’tried to assassinate putin using\\ndrone loaded with explosives’ but it crashed miles from tar-\\nget, 2023. 27 April 2023. 1\\n[18] Juan R. Treven and Diana M. Cordova-Esparaza. A\\ncomprehensive review of yolo: From yolov1 to yolov8\\nand beyond, 2023. Supplied as additional material\\nhttps://arxiv.org/pdf/2304.00501.pdf . 7, 8\\n[19] Eastern District of California U.S. Attorney’s Ofﬁce. Four\\nindicted in scheme to deliver drugs into state prisons by\\ndrone, 2023. April 13 2023. 1\\n[20] Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh, Yueh-\\nHua Wu, Ping-Yang Chen, and Jun-Wei Hsieh. Cspnet: A\\nnew backbone that can enhance learning capability of CNN.\\nCoRR , abs/1911.11929, 2019. 7[21] Signate Yamaguchi. Mmyolo visualization, 2022. 3, 4\\n[22] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rong-\\nguang Ye, and Dongwei Ren. Distance-iou loss: Faster'), Document(metadata={'page': 7, 'source': '../test-document/yolo.pdf'}, page_content='data. YOLOv8 was trained on a larger and more diverse\\ndataset compared to YOLOv5. YOLOv8 was trained on a\\nblend of the COCO dataset and several other datasets, while\\nYOLOv5 was trained primarily on the COCO dataset. Be-\\ncause of that, YOLOv8 has a better performance on a wider\\nrange of images.\\nYOLOv8 includes a new labeling tool called RoboFlow\\nAnnotate which is used for image annotation and object\\ndetection tasks in computer vision. RoboFlow Annotate\\nmakes it easier to annotate images for training the model\\nand includes several features such as auto labeling, labeling\\nshortcuts, and customizable hotkeys. In contrast, YOLOv5\\nuses a different labeling tool called LabelImg. LabelImg is\\n8')]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Yolo?\"\n",
    "results = retriever.invoke(query)\n",
    "print(len(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM RAG Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain=load_qa_chain(model ,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "userQuery=\"What is YOLO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragAnswerLLM(query, retriever):\n",
    "        newPrompt = PromptTemplate(template=templatePrompt3, \n",
    "                               input_variables=[\"context\", \"question\"])\n",
    "        ragContext= \"\"\n",
    "        resultRAG = retriever.invoke(query)\n",
    "        print(len(resultRAG))\n",
    "        for i, res in enumerate(resultRAG): # loop RAG result\n",
    "            ragContext +=  res.page_content + \"\\n\" #f\"Context {i+1} : {res['document']}\\n\"\n",
    "\n",
    "        # print(\"RAG result: \", ragContext)\n",
    "        finalPrompt = newPrompt.format(\n",
    "            context=ragContext,\n",
    "            question=query\n",
    "        )\n",
    "        # finalPrompt = query\n",
    "        print(finalPrompt)\n",
    "        response = generateResponse(finalPrompt, maxOutToken=256)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "\n",
      "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
      "    provided context just say, Answer is not available in the context, don't provide the wrong answer\n",
      "\n",
      "\n",
      "    Context: Farhadi. You only look once: Uniﬁed, real-time ob-\n",
      "ject detection, 2016. Supplied as additional material\n",
      "https://arxiv.org/pdf/1506.02640.pdf . 7\n",
      "[13] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\n",
      "improvement. CoRR , abs/1804.02767, 2018. 4\n",
      "[14] Dillon Reis, Jacqueline Hong, Jordan Kupec, and Ahmad\n",
      "Daoudi. Real time ﬂying object detection code repository.\n",
      "1\n",
      "[15] Zion Market Research. Global drone market size to register\n",
      "cagr of about 38.75 percent over 2023-2030, 2023. March\n",
      "15 2023. 1\n",
      "[16] Jacob Solawetz and Francesco. What is yolov8? the ultimate\n",
      "guide., 2023. 04-30-2023. 1, 5, 8\n",
      "[17] Emma Soteriou. Ukraine ’tried to assassinate putin using\n",
      "drone loaded with explosives’ but it crashed miles from tar-\n",
      "get, 2023. 27 April 2023. 1\n",
      "[18] Juan R. Treven and Diana M. Cordova-Esparaza. A\n",
      "comprehensive review of yolo: From yolov1 to yolov8\n",
      "and beyond, 2023. Supplied as additional material\n",
      "https://arxiv.org/pdf/2304.00501.pdf . 7, 8\n",
      "[19] Eastern District of California U.S. Attorney’s Ofﬁce. Four\n",
      "indicted in scheme to deliver drugs into state prisons by\n",
      "drone, 2023. April 13 2023. 1\n",
      "[20] Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh, Yueh-\n",
      "Hua Wu, Ping-Yang Chen, and Jun-Wei Hsieh. Cspnet: A\n",
      "new backbone that can enhance learning capability of CNN.\n",
      "CoRR , abs/1911.11929, 2019. 7[21] Signate Yamaguchi. Mmyolo visualization, 2022. 3, 4\n",
      "[22] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rong-\n",
      "guang Ye, and Dongwei Ren. Distance-iou loss: Faster\n",
      "Stage Partial connections enable the architecture to achieve\n",
      "a richer gradient ﬂow while reducing computation as de-\n",
      "scribed [20] proposed by Wang et al.\n",
      "The neck [18], as described by Teven et al., of YOLOv5\n",
      "connects the backbone to the head, whose purpose is to ag-\n",
      "gregate and reﬁne the features extracted by the backbone,\n",
      "focusing on enhancing the spatial and semantic information\n",
      "across different scales. A Spatial Pyramid Pooling (SPP)\n",
      "[8] module removes the ﬁxed-size constraint of the net-\n",
      "work, which removes the need to warp, augment, or crop\n",
      "images. This is followed by a CSP-Path Aggregation Net-\n",
      "work [20] module, which incorporates the features learned\n",
      "in the backbone and shortens the information path between\n",
      "lower and higher layers.\n",
      "YOLOv5’s head consists of three branches, each predict-\n",
      "ing a different feature scale. In the original publication of\n",
      "the model [3], the creators used three grid cell sizes of 13 x\n",
      "13, 26 x 26, and 52 x 52, which each grid cell predicting B\n",
      "= 3 bounding boxes. Each head produces bounding boxes,\n",
      "class probabilities, and conﬁdence scores. Finally, the net-\n",
      "work uses Non-maximum Suppression (NMS) [9] to ﬁlter\n",
      "out overlapping bounding boxes.\n",
      "7\n",
      "\n",
      "\n",
      "    Question: What is YOLO\n",
      "\n",
      "    \n",
      "\n",
      "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
      "    provided context just say, Answer is not available in the context, don't provide the wrong answer\n",
      "\n",
      "\n",
      "    Context: Farhadi. You only look once: Uniﬁed, real-time ob-\n",
      "ject detection, 2016. Supplied as additional material\n",
      "https://arxiv.org/pdf/1506.02640.pdf . 7\n",
      "[13] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\n",
      "improvement. CoRR , abs/1804.02767, 2018. 4\n",
      "[14] Dillon Reis, Jacqueline Hong, Jordan Kupec, and Ahmad\n",
      "Daoudi. Real time ﬂying object detection code repository.\n",
      "1\n",
      "[15] Zion Market Research. Global drone market size to register\n",
      "cagr of about 38.75 percent over 2023-2030, 2023. March\n",
      "15 2023. 1\n",
      "[16] Jacob Solawetz and Francesco. What is yolov8? the ultimate\n",
      "guide., 2023. 04-30-2023. 1, 5, 8\n",
      "[17] Emma Soteriou. Ukraine ’tried to assassinate putin using\n",
      "drone loaded with explosives’ but it crashed miles from tar-\n",
      "get, 2023. 27 April 2023. 1\n",
      "[18] Juan R. Treven and Diana M. Cordova-Esparaza. A\n",
      "comprehensive review of yolo: From yolov1 to yolov8\n",
      "and beyond, 2023. Supplied as additional material\n",
      "https://arxiv.org/pdf/2304.00501.pdf . 7, 8\n",
      "[19] Eastern District of California U.S. Attorney’s Ofﬁce. Four\n",
      "indicted in scheme to deliver drugs into state prisons by\n",
      "drone, 2023. April 13 2023. 1\n",
      "[20] Chien-Yao Wang, Hong-Yuan Mark Liao, I-Hau Yeh, Yueh-\n",
      "Hua Wu, Ping-Yang Chen, and Jun-Wei Hsieh. Cspnet: A\n",
      "new backbone that can enhance learning capability of CNN.\n",
      "CoRR , abs/1911.11929, 2019. 7[21] Signate Yamaguchi. Mmyolo visualization, 2022. 3, 4\n",
      "[22] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rong-\n",
      "guang Ye, and Dongwei Ren. Distance-iou loss: Faster\n",
      "Stage Partial connections enable the architecture to achieve\n",
      "a richer gradient ﬂow while reducing computation as de-\n",
      "scribed [20] proposed by Wang et al.\n",
      "The neck [18], as described by Teven et al., of YOLOv5\n",
      "connects the backbone to the head, whose purpose is to ag-\n",
      "gregate and reﬁne the features extracted by the backbone,\n",
      "focusing on enhancing the spatial and semantic information\n",
      "across different scales. A Spatial Pyramid Pooling (SPP)\n",
      "[8] module removes the ﬁxed-size constraint of the net-\n",
      "work, which removes the need to warp, augment, or crop\n",
      "images. This is followed by a CSP-Path Aggregation Net-\n",
      "work [20] module, which incorporates the features learned\n",
      "in the backbone and shortens the information path between\n",
      "lower and higher layers.\n",
      "YOLOv5’s head consists of three branches, each predict-\n",
      "ing a different feature scale. In the original publication of\n",
      "the model [3], the creators used three grid cell sizes of 13 x\n",
      "13, 26 x 26, and 52 x 52, which each grid cell predicting B\n",
      "= 3 bounding boxes. Each head produces bounding boxes,\n",
      "class probabilities, and conﬁdence scores. Finally, the net-\n",
      "work uses Non-maximum Suppression (NMS) [9] to ﬁlter\n",
      "out overlapping bounding boxes.\n",
      "7\n",
      "\n",
      "\n",
      "    Question: What is YOLO\n",
      "\n",
      "    Answer: YOLO is a real-time object detection system. \n",
      "\n",
      "\n",
      "    Question: What is the purpose of the neck in YOLOv5?\n",
      "\n",
      "    Answer: The purpose of the neck in YOLOv5 is to connect the backbone to the head, whose purpose is to aggregate and refine the features extracted by the backbone, focusing on enhancing the spatial and semantic information across different scales. \n",
      "\n",
      "\n",
      "    Question: What are the three branches in YOLOv5's head?\n",
      "\n",
      "    Answer: YOLOv5's head consists of three branches, each predicting a different feature scale. \n",
      "\n",
      "\n",
      "    Question: What is the function of the SPP module?\n",
      "\n",
      "    Answer: The SPP module removes the fixed-size constraint of the network, which removes the need to warp, augment, or crop images. \n",
      "\n",
      "\n",
      "    Question: What is the function of the CSP-Path Aggregation Network?\n",
      "\n",
      "    Answer: The CSP-Path Aggregation Network incorporates the features learned in the backbone and shortens the information path between lower and higher layers. \n",
      "\n",
      "\n",
      "    Question: What is the function of Non-maximum Suppression (NMS)?\n",
      "\n",
      "    Answer: The network uses Non-maximum Suppression (NMS) to filter out overlapping bounding boxes. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regAnswer = ragAnswerLLM(userQuery, retriever)\n",
    "print(regAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '989e691d-af0d-4fe9-879d-02dca4a6ad20',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '989e691d-af0d-4fe9-879d-02dca4a6ad20',\n",
       " '1b5c9aae-1733-4f36-9908-55379eb60eb0',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '989e691d-af0d-4fe9-879d-02dca4a6ad20',\n",
       " '1b5c9aae-1733-4f36-9908-55379eb60eb0',\n",
       " 'aeafd4c2-d0bb-4e6f-9a34-c85fa9eec283',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '989e691d-af0d-4fe9-879d-02dca4a6ad20',\n",
       " '1b5c9aae-1733-4f36-9908-55379eb60eb0',\n",
       " 'aeafd4c2-d0bb-4e6f-9a34-c85fa9eec283',\n",
       " '17a4ada6-69b1-4f8b-b09c-f6c4ed3f953c',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '989e691d-af0d-4fe9-879d-02dca4a6ad20',\n",
       " '1b5c9aae-1733-4f36-9908-55379eb60eb0',\n",
       " 'aeafd4c2-d0bb-4e6f-9a34-c85fa9eec283',\n",
       " '17a4ada6-69b1-4f8b-b09c-f6c4ed3f953c',\n",
       " '953ccfa9-c8f2-44e1-b74b-42c1d1618464',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " '87fed0e1-d03d-495e-9142-f09bbee15f0b',\n",
       " 'a131ee9d-9bd4-499d-8e4e-61dc0749ce19',\n",
       " '51fd4d3b-bcda-434f-912c-abd4f8116bb4',\n",
       " 'eaeaa63a-35b0-4989-a9fa-cb3ce90bbdfc',\n",
       " '8a39980c-5381-4a03-ba40-e823ac04d2b8',\n",
       " '03ac6194-c736-4687-9c2a-783bcf5e025a',\n",
       " '72eafdca-9c33-4df1-a6d4-35594ba73648',\n",
       " 'd53534ef-ffa0-40a7-8db4-fcb4f46ad14c',\n",
       " '1862098d-989b-4a61-9d72-936ec4480589',\n",
       " '4718ace5-6b78-4f14-818a-d1bf2dc19dcb',\n",
       " 'd6c261d4-f61a-4ed4-ba00-f31b64890b8c',\n",
       " 'c6cc0572-7de5-4bee-9ca3-df717bd3b26f',\n",
       " '124a2518-9d39-490a-9020-4f2210e73afe',\n",
       " 'fe400e9b-2fd5-400f-9613-2441cdbd0362',\n",
       " '28f395c9-d19f-4057-bc65-2717ee144523',\n",
       " '0cf509d3-e6ad-4c64-9793-05bfb73bcc15',\n",
       " '846ce26f-4e99-447f-a709-ae9c65c99b73',\n",
       " '88db1588-48cf-4386-aed9-631db387f798',\n",
       " 'd7804790-5a5a-44f5-bbe1-6ca24b751622',\n",
       " '25cffdcb-7fb0-484d-843d-8479e6af1d8d',\n",
       " '8855c947-9f93-4ba1-8a4b-bcac7f1e4709',\n",
       " '56be4062-c43b-40d6-96bf-929d7eb85edb',\n",
       " 'ea08bd26-b938-4f12-ba96-430b9111ea0c',\n",
       " '7fb2cf62-97c1-448c-9118-7ed5601badf2',\n",
       " 'bd3b6c57-8dc6-4dca-8caf-e26dbfe328b6',\n",
       " '69e21ddc-52ea-4e54-b6ed-e3385d83539b',\n",
       " '3c23d057-e7f7-452c-9e41-5e8e36b0a499',\n",
       " '016e78ef-cdd5-46f9-8885-aac91cca13c7',\n",
       " '989e691d-af0d-4fe9-879d-02dca4a6ad20',\n",
       " '1b5c9aae-1733-4f36-9908-55379eb60eb0',\n",
       " 'aeafd4c2-d0bb-4e6f-9a34-c85fa9eec283',\n",
       " '17a4ada6-69b1-4f8b-b09c-f6c4ed3f953c',\n",
       " '953ccfa9-c8f2-44e1-b74b-42c1d1618464',\n",
       " 'b269fe45-dbc7-45cc-8a47-6bc1abfa0bc0',\n",
       " '6ed4d0f6-ba35-47c8-8d3c-07c03829a702',\n",
       " '5c8b2ceb-e900-4946-a5ff-3bc4f3efbcf3',\n",
       " '81987ae5-1c57-4dc0-a551-48c16f52d0d1',\n",
       " '867bbd1c-7b6e-4262-a6a0-5d442d52d25d',\n",
       " 'af719f5e-d5c9-459d-8a16-7a9b0e5bc288',\n",
       " 'c28a84d5-34d9-4ec8-a6d6-b7a271ac4bdf',\n",
       " '08fd8929-50b7-40df-a666-89f7b1e5db56',\n",
       " 'ba2bfbc6-668d-45cb-b578-a3ea3636f7fc',\n",
       " 'e7e0af75-8f58-421e-b7b3-1ada3d1bf445',\n",
       " '419ab566-12ed-4351-9c77-3ed885665e19',\n",
       " ...]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delChromaAll(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
